# =============================================================================
# Ollama - Run large language models locally
# =============================================================================
#
# Ollama is a tool for running large language models (LLMs) locally.
# It provides:
# - Easy model management (pull, run, create)
# - REST API for model inference
# - Support for popular models (Llama, Mistral, Code Llama, etc.)
# - GPU acceleration with CUDA and ROCm
#
# Usage:
#   ollama pull llama3.2       # Download a model
#   ollama run llama3.2        # Run interactive chat
#   ollama serve               # Start API server
#
# =============================================================================

load("//defs:package_defs.bzl", "download_source", "binary_package")

download_source(
    name = "ollama-src",
    src_uri = "https://github.com/ollama/ollama/releases/download/v0.4.6/ollama-linux-amd64.tgz",
    sha256 = "0634c55ef05afcce9fc5c8a58a56a6e927c0ad9b0bf50a618c16a8414a1b08e7",
)

binary_package(
    name = "ollama",
    source = ":ollama-src",
    version = "0.4.6",
    description = "Get up and running with large language models locally",
    homepage = "https://ollama.ai/",
    license = "MIT",
    install_script = """
# Install ollama binary
mkdir -p "$DESTDIR/usr/bin"
install -m 755 bin/ollama "$DESTDIR/usr/bin/ollama"

# Create ollama user home directory for models
mkdir -p "$DESTDIR/var/lib/ollama"

# Create systemd service file
mkdir -p "$DESTDIR/etc/systemd/system"
cat > "$DESTDIR/etc/systemd/system/ollama.service" << 'EOF'
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="HOME=/var/lib/ollama"
Environment="OLLAMA_HOST=0.0.0.0"

[Install]
WantedBy=default.target
EOF

# Create default environment configuration
mkdir -p "$DESTDIR/etc/ollama"
cat > "$DESTDIR/etc/ollama/ollama.conf" << 'EOF'
# Ollama configuration
# OLLAMA_HOST=0.0.0.0:11434
# OLLAMA_MODELS=/var/lib/ollama/models
# OLLAMA_NUM_PARALLEL=1
# OLLAMA_MAX_LOADED_MODELS=1
EOF
""",
    visibility = ["PUBLIC"],
)
